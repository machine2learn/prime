{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "matplotlib.use('module://ipykernel.pylab.backend_inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import statsmodels.api as sm\n",
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.use('module://ipykernel.pylab.backend_inline')\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_difference(df_source, i):\n",
    "    assignment = {}\n",
    "    assignment[f\"time_difference.0.{i}\"] = (pd.to_datetime(df_source[f\"date_visit.{i}\"]) - pd.to_datetime(df_source[\"date_visit.0\"])).dt.total_seconds() / (24 * 3600)\n",
    "    df_source = df_source.assign(**assignment)\n",
    "    return df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples =None\n",
    "memory_experiment = \"pm\"\n",
    "compute_simple_models = False\n",
    "\n",
    "\n",
    "binary_smoke = False\n",
    "\n",
    "second_visit = 2\n",
    "lasso_epsilon = 0.01\n",
    "p_val = 0.05\n",
    "age_limit = 0\n",
    "gender = None\n",
    "iqr_coefficient = None # if None then no standrat removal of outliers\n",
    "\n",
    "keep_ldl_hdl = True # if lasso selects only one of them keep the second anyway; they are correlated but have different influence\n",
    "quadr_on_log_if_skewed = True\n",
    "\n",
    "HSCD = 'HC1' # if none than the model builder will be heteroscedascisity unaware\n",
    "\n",
    "input_dir = \"/projects/prime/ukbb/preprocessed_data_2024\"\n",
    "imputed_dir = f\"/projects/prime/ukbb/preprocessed_data_2024/imputations/ukbb_imputed_{memory_experiment}_sg_0_{second_visit}_rf/ukbb_imputed_{memory_experiment}_sg_0_{second_visit}_rf_instance_1\"\n",
    "path_imputed =  f\"{imputed_dir}/ukbb_imputed_{memory_experiment}_sg_0_{second_visit}_rf_instance_1.csv\"\n",
    "input_filenames = {\n",
    "    \"diet\": \"diet_2024_bradbury_march.csv\",\n",
    "    \"alcohol\": \"alcohol_2024_maart.csv\",\n",
    "    \"diagnoses\": \"diagnoses_2024.csv\",\n",
    "    \"lifestyle\": \"participants_age_date_smoke_MET_2024_jan.csv\",\n",
    "    \"medications\": \"participants_medications_2024_jan.csv\",\n",
    "    \"risk_factors\": \"blood_count_biochemistry_pressure_maart_2024.csv\",\n",
    "    \"sociodemo\": \"edu_job_marriage_2024.csv\",\n",
    "    \"cognition\": \"cognitive_2024_maart.csv\",\n",
    "    \"metrics\": \"body_size_measures_gender_dates_2024_jan.csv\"\n",
    "  }\n",
    "\n",
    "output_dir =f\"/projects/prime/ukbb/results_2024/{memory_experiment}_sg_0_{second_visit}_no_interactions_quadr_hdl_ldl_remove_skewed_originals_27_05_nutr_only\"\n",
    "if HSCD is not None:\n",
    "  output_dir = f\"{output_dir}_{HSCD}\"\n",
    "\n",
    "if gender is None:\n",
    "  output_dir = f\"{output_dir}/\"\n",
    "else:\n",
    "  output_dir = f\"{output_dir}_gender_{gender}/\"\n",
    "if os.path.exists(output_dir):\n",
    "   shutil.rmtree(output_dir)\n",
    "    # The directory does not exist, create it\n",
    "os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if memory_experiment == \"nm\":\n",
    "    memory_experiment_full_name = \"num_memory_max_digits_remembered_correctly\"\n",
    "else:\n",
    "    memory_experiment_full_name = \"pairs_matching_sum_incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "predictors = [\"gender.0\", f\"time_difference.0.{second_visit}\", \"Diabetes_2.0\",  \"Dyslipidemia.0\", \"Depression.0\", \"Hypertension.0\", \n",
    "  \"age_years.0\", \"education_level.0\", \"marital_status.0\", \"pp_smoke_catgeory.0\", \"MET_score.0\", \"waist_cm.0\",  \"bmi_kg_m2.0\", \n",
    "  \"Syst_bp.0\", \"Diast_bp.0\", \"ldl_conv.0\", \"hdl_conv.0\", \"Triglycerides_conv.0\", \"HbA1c_conv.0\",  \"Pl_glucose_conv.0\",   \"Albumin.0\", \n",
    "  \"Leukocyte_count.0\", \"C_reactive_protein.0\",\n",
    "   \"oily_fish_gpday_bradbury.0\", \"white_fish_gpday_bradbury.0\", \n",
    "   \"red_meat_bradbury_gpd.0\", \"poultry_gpday_bradbury.0\",\n",
    "   \"processed_meat_gpday_bradbury.0\", \"veg_gpday_bradbury.0\",\n",
    "   \"fruit_gpday_bradbury.0\",\n",
    "    \"cereals_gpday_bradbury.0\", \"bread_gpday_bradbury.0\", \"cheese_gpday_bradbury.0\",\n",
    "    \"milk_gpday_bradbury.0\", \"tea_gpday_bradbury.0\",\n",
    "    \"red_wine_gpd.0\", \"white_wine_gpd.0\", \"fortified_gpd.0\", \"beer_cider_gpd.0\", \"spirits_gpd.0\",\n",
    "   \"aspirin.0\",  \"anxiety_tr.0\", \"pain_tr.0\", \"TAZD_Thiazide.0\",  \n",
    "   \"loop_diuretics.0\",  \"potassium_diuretics.0\", \"beta_blockers.0\", \"calcium_antagonists.0\", \n",
    "   \"ARA_II_Antagonists_of_angiotensin_II_receptors.0\", \"IECA_Angiotensin_converting_enzyme_inhibitors.0\",\n",
    "   \"Other_Hypotensive.0\",\"hypochol_statins.0\", \"hypochol_others.0\", \"insulin.0\", \"sulfonylurea.0\", \"thiazolidinediones.0\", \"non_sulfonylurea_insulin_secretagogues.0\", \"metformin_category.0\", \"vitamins_minerals.0\"]\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [\"gender.0\", \"age_years.0\", \"education_level.0\", f\"time_difference.0.{second_visit}\",\n",
    "   \"oily_fish_gpday_bradbury.0\", \"white_fish_gpday_bradbury.0\", \n",
    "   \"red_meat_bradbury_gpd.0\", \"poultry_gpday_bradbury.0\",\n",
    "   \"processed_meat_gpday_bradbury.0\", \"veg_gpday_bradbury.0\",\n",
    "   \"fruit_gpday_bradbury.0\",\n",
    "    \"cereals_gpday_bradbury.0\", \"bread_gpday_bradbury.0\", \"cheese_gpday_bradbury.0\",\n",
    "    \"milk_gpday_bradbury.0\", \"tea_gpday_bradbury.0\",\n",
    "    \"red_wine_gpd.0\", \"white_wine_gpd.0\", \"fortified_gpd.0\", \"beer_cider_gpd.0\", \"spirits_gpd.0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_to_plot = [f\"time_difference.0.{second_visit}\", \n",
    "  \"age_years.0\", \"MET_score.0\", \"waist_cm.0\",  \"bmi_kg_m2.0\", \n",
    "  \"Syst_bp.0\", \"Diast_bp.0\", \"ldl_conv.0\", \"hdl_conv.0\", \"Triglycerides_conv.0\", \"HbA1c_conv.0\",  \"Pl_glucose_conv.0\",   \"Albumin.0\", \n",
    "  \"Leukocyte_count.0\", \"C_reactive_protein.0\",\n",
    "   \"oily_fish_gpday_bradbury.0\", \"white_fish_gpday_bradbury.0\", \n",
    "   \"red_meat_bradbury_gpd.0\", \"poultry_gpday_bradbury.0\",\n",
    "   \"processed_meat_gpday_bradbury.0\", \"veg_gpday_bradbury.0\",\n",
    "   \"fruit_gpday_bradbury.0\",\n",
    "    \"cereals_gpday_bradbury.0\", \"bread_gpday_bradbury.0\", \"cheese_gpday_bradbury.0\",\n",
    "    \"milk_gpday_bradbury.0\", \"tea_gpday_bradbury.0\",\n",
    "    \"red_wine_gpd.0\", \"white_wine_gpd.0\", \"fortified_gpd.0\", \"beer_cider_gpd.0\", \"spirits_gpd.0\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_standart = [\"Syst_bp.0\", \"Diast_bp.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_predictors =[\"age_years.0\", \"gender.0\", \"education_level.0\", f\"log_time_difference.0.{second_visit}\"]\n",
    "if (gender is not None) and (\"gender.0\") in simple_predictors:\n",
    "    simple_predictors.remove(\"gender.0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_components = [f\"{memory_experiment_full_name}.0\", f\"{memory_experiment_full_name}.{second_visit}\", \n",
    "            \"snap_game_true_pos_rt_avrg.0\", f\"snap_game_true_pos_rt_avrg.{second_visit}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_features = [f\"time_difference.0.{second_visit}\", \"age_years.0\", \"MET_score.0\", \n",
    "  \"Triglycerides_conv.0\", \"HbA1c_conv.0\",  \"Pl_glucose_conv.0\",   \n",
    "  \"Leukocyte_count.0\", \"C_reactive_protein.0\",\n",
    "  \"red_wine_gpd.0\", \"white_wine_gpd.0\", \"fortified_gpd.0\", \"beer_cider_gpd.0\", \"spirits_gpd.0\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadr_features = [\"MET_score.0\", \"waist_cm.0\",  \"bmi_kg_m2.0\", \n",
    "  \"Syst_bp.0\", \"Diast_bp.0\", \"ldl_conv.0\", \"hdl_conv.0\", \"Triglycerides_conv.0\", \"HbA1c_conv.0\",  \"Pl_glucose_conv.0\",   \"Albumin.0\", \n",
    "  \"Leukocyte_count.0\", \"C_reactive_protein.0\",\n",
    "   \"oily_fish_gpday_bradbury.0\", \"white_fish_gpday_bradbury.0\", \n",
    "   \"red_meat_bradbury_gpd.0\", \"poultry_gpday_bradbury.0\",\n",
    "   \"processed_meat_gpday_bradbury.0\", \"veg_gpday_bradbury.0\",\n",
    "   \"fruit_gpday_bradbury.0\",\n",
    "    \"cereals_gpday_bradbury.0\", \"bread_gpday_bradbury.0\", \"cheese_gpday_bradbury.0\",\n",
    "    \"milk_gpday_bradbury.0\", \"tea_gpday_bradbury.0\"\n",
    "   #  \"red_wine_gpd.0\", \"white_wine_gpd.0\", \"fortified_gpd.0\", \"beer_cider_gpd.0\", \"spirits_gpd.0\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''outliers = {\n",
    "    \"ldl_conv.0\": [0, 290],\n",
    "    \"Syst_bp.0\": [0, 220],\n",
    "}'''\n",
    "outliers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "interaction_lists = []\n",
    "interaction_pairs = []\n",
    "\n",
    "# very skewed:\n",
    "remove_original_features = [\"MET_score.0\", \"Triglycerides_conv.0\", \"Leukocyte_count.0\", \"C_reactive_protein.0\",\n",
    "                            \"red_wine_gpd.0\", \"white_wine_gpd.0\", \"fortified_gpd.0\", \"beer_cider_gpd.0\", \"spirits_gpd.0\"\n",
    "                            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_needed = [\"f.eid\"] + predictors + outcome_components\n",
    "for i in range(1,4):\n",
    "    if f\"time_difference.0.{i}\" in predictors: # will be calculate from dates\n",
    "        all_needed.remove(f\"time_difference.0.{i}\")\n",
    "        all_needed = all_needed + [f\"date_visit.{i}\"] \n",
    "        if \"date_visit.0\" not in all_needed:\n",
    "             all_needed = all_needed + [\"date_visit.0\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data set and select all not NA\n",
    "df_index = {}\n",
    "first = True\n",
    "for label, filename in input_filenames.items():\n",
    "    print(label)\n",
    "    full_path = f\"{input_dir}/{filename}\"\n",
    "    \n",
    "    data_columns = pd.read_table(full_path, nrows=1, sep=',').columns\n",
    "    \n",
    "    needed_fields = data_columns[data_columns.isin(all_needed)]\n",
    "    \n",
    "    if label == \"medications\":\n",
    "        needed_fields = needed_fields.tolist() + [\"alpha_glucosidase_inhibitors.0\"] \n",
    "        \n",
    "    df_index[label] = pd.read_table(full_path, sep=',', usecols=needed_fields, dtype=str, nrows=n_samples)\n",
    "    \n",
    "    \n",
    "    print(len(df_index[label]))\n",
    "    \n",
    "    for clmn in needed_fields:\n",
    "        if clmn == \"f.eid\" or clmn.startswith(\"date_visit\"):\n",
    "            continue\n",
    "        df_index[label][clmn] = df_index[label][clmn].astype(float)\n",
    "        \n",
    "    \n",
    "    if first:\n",
    "        df = df_index[label]\n",
    "        first = False\n",
    "    else:\n",
    "        df = pd.merge(df,df_index[label])\n",
    "        \n",
    "    print(len(df))\n",
    "\n",
    "\n",
    "\n",
    "# remove NA from the check up data set \n",
    "df = df.dropna(axis=\"rows\")\n",
    "print(f\"cleaned check up db has {len(df)} rows\")\n",
    "\n",
    "df= df.loc[df[\"alpha_glucosidase_inhibitors.0\"] != 1]\n",
    "print(\"removed all participants with alpha_glucosidase_inhibitors.0 == 1:\")  \n",
    "df = df.drop(\"alpha_glucosidase_inhibitors.0\", axis = 1)\n",
    "print(len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load imputed data set\n",
    "df_imputed = pd.read_table(path_imputed, sep=',', usecols=all_needed, dtype=str, nrows=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all columns if the imputed except dates, float\n",
    "for clmn in df_imputed.columns:\n",
    "    if clmn == \"f.eid\" or clmn.startswith(\"date_visit\"):\n",
    "        continue\n",
    "    df_imputed[clmn] = df_imputed[clmn].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter both data set by age limits (which can be zero)\n",
    "if \"age_years.0\" in df.columns:\n",
    "    df= df[df[\"age_years.0\"].ge(age_limit)]\n",
    "    print(\"removed all participants younger age_limits no NA:\") \n",
    "    print(len(df))\n",
    "\n",
    "if \"age_years.0\" in df_imputed.columns:\n",
    "    df_imputed= df_imputed[df_imputed[\"age_years.0\"].ge(age_limit)]\n",
    "    print(\"removed all participants younger age_limits imputed:\") \n",
    "    print(len(df_imputed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter both data sets by gender if required\n",
    "if gender is not None and \"gender.0\" in df.columns and \"gender.0\" in df_imputed.columns:\n",
    "    df= df.loc[df[\"gender.0\"].eq(gender)]\n",
    "    df.drop(columns=['gender.0'])\n",
    "    print(f\"only participants with gender {gender} are left in the not na data set: \") \n",
    "    \n",
    "    print(len(df)) \n",
    "    \n",
    "    df_imputed= df_imputed.loc[df_imputed[\"gender.0\"].eq(gender)]\n",
    "    df_imputed.drop(columns=['gender.0'])\n",
    "    print(f\"only participants with gender {gender} are left in the omputed data set: \") \n",
    "    print(len(df_imputed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to both data data sets time difference column\n",
    "for i in range(1,4):\n",
    "    if f\"time_difference.0.{i}\" in predictors:\n",
    "        df = add_time_difference(df, i)   \n",
    "        df_imputed = add_time_difference(df_imputed, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to both data sets binarised smoke score \n",
    "if binary_smoke:\n",
    "    \n",
    "    ind_replace = predictors.index(\"smoking_score.0\")\n",
    "    predictors[ind_replace] = \"binary_smoking_score.0\"\n",
    "    \n",
    "    assignment = {}\n",
    "    assignment[\"binary_smoking_score.0\"] =np.where(df[\"smoking_score.0\"] == 2,1, 0) \n",
    "    df = df.assign(**assignment)\n",
    "\n",
    "    assignment = {}\n",
    "    assignment[\"binary_smoking_score.0\"] =np.where(df_imputed[\"smoking_score.0\"] == 2,1, 0) \n",
    "    df_imputed = df_imputed.assign(**assignment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove from both data sets customized outliers\n",
    "for feat, out_ in outliers.items():\n",
    "   df_imputed= df_imputed[(df_imputed[feat] >= out_[0]) & (df_imputed[feat] <= out_[1])]\n",
    "   print(f\"data set imputed after removing customized outliers: {len(df_imputed)}\")\n",
    "   df= df[df[feat] >= out_[0] & df[feat] <= out_[1]]\n",
    "   print(f\"data set no na after removing customized outliers: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add log features \n",
    "def add_logs(df_, log_features, outcome_components, descr):\n",
    "    print(f\"adding logarithmic features to {descr}\")\n",
    "    assignment = {}\n",
    "    log_cols = []\n",
    "\n",
    "    for log_feature in log_features:\n",
    "        if log_feature in df_.columns:\n",
    "            new_fld = f\"log_{log_feature}\"\n",
    "            assignment[new_fld] = np.log1p(df_[log_feature])\n",
    "            log_cols = log_cols + [new_fld]\n",
    "            print(new_fld)\n",
    "    \n",
    "    for i in range(0,4):\n",
    "        \n",
    "        if f\"pairs_matching_sum_incorrect.{i}\" in outcome_components:\n",
    "            new_fld = f\"log_pairs_matching_sum_incorrect.{i}\"\n",
    "            if new_fld not in assignment:\n",
    "                assignment[new_fld] = np.log1p(df_[f\"pairs_matching_sum_incorrect.{i}\"])\n",
    "                print(new_fld)\n",
    "                \n",
    "        \n",
    "        if f\"pairs_matching_incorrect_in_round_1.{i}\" in outcome_components:\n",
    "            new_fld_round = f\"log_pairs_matching_incorrect_in_round_1.{i}\"\n",
    "            if new_fld_round not in assignment:\n",
    "                assignment[new_fld_round] = np.log1p(df_[f\"pairs_matching_incorrect_in_round_1.{i}\"])  \n",
    "                print(new_fld) \n",
    "\n",
    "    df_ = df_.assign(**assignment)\n",
    "    return df_, log_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, log_cols =  add_logs(df, log_features, outcome_components, \"check up db with no NA\")\n",
    "df_imputed, _  =  add_logs(df_imputed, log_features, outcome_components,\"df with imputed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to quadr features \n",
    "def add_squares(df_, q_features, descr, skewed_only_on_log, skewed_features):\n",
    "    print(f\"adding quadratic features to {descr}\")\n",
    "    assignment = {}\n",
    "    quadr_cols = []\n",
    "    \n",
    "    for feat in df_.columns:\n",
    "        for q_feat in q_features:\n",
    "            if q_feat in feat:\n",
    "                if not skewed_only_on_log or (\"log_\" in feat) or not (feat in skewed_features):\n",
    "                    new_fld = f\"quadr_{feat}\"\n",
    "                    assignment[new_fld] = np.square(df_[feat])\n",
    "                    quadr_cols = quadr_cols + [new_fld]\n",
    "                    print(new_fld)\n",
    "    \n",
    "    \n",
    "\n",
    "    df_ = df_.assign(**assignment)\n",
    "    return df_, quadr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, quad_cols =  add_squares(df, quadr_features, \"check up db with no NA\", quadr_on_log_if_skewed, remove_original_features)\n",
    "df_imputed, _  =  add_squares(df_imputed, quadr_features, \"df with imputed\", quadr_on_log_if_skewed, remove_original_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for removing outliers via iqr approach\n",
    "def remove_outliers_iqr(df_, iqr_cf, df_name):\n",
    "    if iqr_cf is not None:\n",
    "        \n",
    "        lower_bound = {}\n",
    "        upper_bound = {}\n",
    "        \n",
    "        for feat  in outliers_standart:\n",
    "            print(f\"{feat}\")\n",
    "            \n",
    "            Q1 = df_[feat].quantile(0.25)\n",
    "            Q3 = df_[feat].quantile(0.75)\n",
    "            \n",
    "            IQR = Q3-Q1\n",
    "            lower_bound[feat] = Q1 - iqr_cf * IQR\n",
    "            upper_bound[feat] = Q3 + iqr_cf * IQR\n",
    "            \n",
    "            if feat in log_features:\n",
    "                feat=f\"log_{feat}\"\n",
    "                print(f\"logarithimic outlier {feat}\")\n",
    "            \n",
    "                Q1 = df_[feat].quantile(0.25)\n",
    "                Q3 = df_[feat].quantile(0.75)\n",
    "                \n",
    "                IQR = Q3-Q1\n",
    "                lower_bound[feat] = Q1 - iqr_cf * IQR\n",
    "                upper_bound[feat] = Q3 + iqr_cf * IQR\n",
    "            \n",
    "            \n",
    "                \n",
    "         \n",
    "            \n",
    "        for feat in lower_bound:\n",
    "            df_= df_[(df_[feat] >= lower_bound[feat]) & (df_[feat] <= upper_bound[feat])]\n",
    "            \n",
    "        print(f\"data set {df_name} after removing outliers: {len(df_)}\")\n",
    "    return(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers via iqr approach\n",
    "df = remove_outliers_iqr(df, iqr_coefficient, \"no na\")\n",
    "df_imputed = remove_outliers_iqr(df_imputed, iqr_coefficient, \"imputed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which adds the column with the RV scores to the data set\n",
    "def adding_rv_scores(df_source, mem_experiment_full, repeat):\n",
    "    assignment = {}\n",
    "    weight_mem = 1\n",
    "    if \"pairs_matching\" in mem_experiment_full:\n",
    "        weight_mem = -1\n",
    "        \n",
    "    sum_mem_0 = df_source[f\"{mem_experiment_full}.0\"].sum()\n",
    "    mem_mean_0 = df_source[f\"{mem_experiment_full}.0\"].mean()\n",
    "    mem_std_0 = df_source[f\"{mem_experiment_full}.0\"].std()\n",
    "    assignment[f'z_{mem_experiment_full}.0'] = (df_source[f\"{mem_experiment_full}.0\"] - mem_mean_0)/mem_std_0\n",
    "    \n",
    "    sum_mem_repeat = df_source[f\"{mem_experiment_full}.{repeat}\"].sum()\n",
    "    mem_mean_repeat = df_source[f\"{mem_experiment_full}.{repeat}\"].mean()\n",
    "    mem_std_repeat = df_source[f\"{mem_experiment_full}.{repeat}\"].std()\n",
    "    assignment[f'z_{mem_experiment_full}.{repeat}'] = (df_source[f\"{mem_experiment_full}.{repeat}\"] - mem_mean_0)/mem_std_0 \n",
    "    \n",
    "    sum_sg_rt_mean_0 = df_source[\"snap_game_true_pos_rt_avrg.0\"].sum()\n",
    "    sg_rt_mean_0 = df_source[\"snap_game_true_pos_rt_avrg.0\"].mean()\n",
    "    sg_rt_std_0 = df_source[\"snap_game_true_pos_rt_avrg.0\"].std()\n",
    "    assignment['z_snap_game_true_pos_rt_avrg.0'] = (df_source[\"snap_game_true_pos_rt_avrg.0\"] - sg_rt_mean_0)/sg_rt_std_0\n",
    "\n",
    "    sum_sg_rt_mean_repeat = df_source[f\"snap_game_true_pos_rt_avrg.{repeat}\"].sum()\n",
    "    sg_rt_mean_repeat = df_source[f\"snap_game_true_pos_rt_avrg.{repeat}\"].mean() \n",
    "    sg_rt_std_repeat = df_source[f\"snap_game_true_pos_rt_avrg.{repeat}\"].std()\n",
    "    assignment[f'z_snap_game_true_pos_rt_avrg.{repeat}'] = (df_source[f\"snap_game_true_pos_rt_avrg.{repeat}\"] - sg_rt_mean_0)/sg_rt_std_0 \n",
    "    \n",
    "     # generate Global_v00 = (zDSTf_v00 + zDSTb_v00 - zTMTa_v00 - zTMTb_v00) / 4\n",
    "    assignment[\"global.0\"] = (- assignment['z_snap_game_true_pos_rt_avrg.0'] + weight_mem * assignment[f'z_{mem_experiment_full}.0'])/2.0\n",
    "    sum_global_0 = assignment[\"global.0\"].sum()\n",
    "    mean_global_0 = assignment[\"global.0\"].mean()\n",
    "    std_global_0 = assignment[\"global.0\"].std()\n",
    "    assignment['z_global.0'] = (assignment[\"global.0\"] - mean_global_0)/std_global_0\n",
    "\n",
    "    assignment[f\"global.{repeat}\"] = (- assignment[f'z_snap_game_true_pos_rt_avrg.{repeat}'] + weight_mem * assignment[f'z_{mem_experiment_full}.{repeat}'])/2.0 \n",
    "    assignment[f\"z_global.{repeat}\"] = (assignment[f\"global.{repeat}\"] - mean_global_0)/std_global_0\n",
    "    \n",
    "   \n",
    "    \n",
    "    assignment[f\"z_change_{mem_experiment_full}.0.{repeat}\"] = assignment[f'z_{mem_experiment_full}.{repeat}'] - assignment[f'z_{mem_experiment_full}.0']\n",
    "    assignment[f\"z_change_snap_game_true_pos_rt_avrg.0.{repeat}\"] = assignment[f'z_snap_game_true_pos_rt_avrg.{repeat}'] - assignment['z_snap_game_true_pos_rt_avrg.0']\n",
    "    df_source = df_source.assign(**assignment) \n",
    "    return df_source    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if memory_experiment == \"pm\":\n",
    "    memory_experiment = f\"log_{memory_experiment}\"\n",
    "    memory_experiment_full_name = f\"log_{memory_experiment_full_name}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = adding_rv_scores(df, memory_experiment_full_name, second_visit)\n",
    "df_imputed = adding_rv_scores(df_imputed, memory_experiment_full_name, second_visit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# these files are used to test validator.R\n",
    "df.to_csv(f\"{output_dir}/test_non_na.csv\", sep=',')\n",
    "df_imputed.to_csv(f\"{output_dir}/test_ukbb_imputed_{memory_experiment}_sg_0_{second_visit}_rf_instance_1.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe population\n",
    "def describe_population(df_, descr): \n",
    "    print(descr)\n",
    "    comorbidity = [\"Diabetes_2.0\",  \"Dyslipidemia.0\", \"Depression.0\", \"Hypertension.0\"]\n",
    "    n_comor = {}\n",
    "    percentage_comor = {}\n",
    "    n_comor_female = {}\n",
    "    percentage_comor_female = {}\n",
    "    for comor in comorbidity:\n",
    "        if comor in df_.columns: \n",
    "            n_comor[comor] = df_[comor].sum()\n",
    "            percentage_comor[comor] = (float(n_comor[comor])*100)/float(len(df_))\n",
    "            print(f\"The % of participants with {comor} is {percentage_comor[comor]}\")\n",
    "            if \"gender.0\" in df_.columns:\n",
    "                n_comor_female[comor] = (df_[comor] + df_[\"gender.0\"]).eq(2.0).sum()\n",
    "                percentage_comor_female[comor] = (float(n_comor_female[comor])*100)/float(len(df_))\n",
    "                print(f\"The % of female participants with {comor} is {percentage_comor_female[comor]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_population(df, \"check up df with no NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_population(df_imputed, \"df with imputations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add interactions\n",
    "def add_interactions(df_, interaction_lists, interaction_pairs, descr):\n",
    "    if len(interaction_lists) == 0 and len(interaction_pairs) == 0:\n",
    "        return df_, []\n",
    "    print(f\"adding interaction  features to {descr}\")\n",
    "    assignment = {}\n",
    "    interaction_cols = []\n",
    "\n",
    "    for int_1 in interaction_lists[0]:\n",
    "        print(int_1)\n",
    "        for int_2 in interaction_lists[1]:\n",
    "            print(int_2)\n",
    "            new_fld = f\"{int_1}_x_{int_2}\"\n",
    "            if new_fld not in df_.columns:\n",
    "                assignment[new_fld] = df_[int_1] * df_[int_2]\n",
    "                interaction_cols = [new_fld] + interaction_cols\n",
    "    df_ = df_.assign(**assignment) \n",
    "     \n",
    "    assignment = {}\n",
    "    print(df_.columns)\n",
    "    for interaction_pair in interaction_pairs:\n",
    "        new_fld = f\"{interaction_pair[0]}_x_{interaction_pair[1]}\"\n",
    "        if new_fld not in df_.columns:\n",
    "            assignment[new_fld] = df_[interaction_pair[0]] * df_[interaction_pair[1]]\n",
    "            print(new_fld)\n",
    "            interaction_cols = [new_fld] + interaction_cols\n",
    "    df_ = df_.assign(**assignment)   \n",
    "    return df_, interaction_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, interaction_cols =  add_interactions(df, interaction_lists, interaction_pairs, \"check up db with no NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed, _  =  add_interactions(df_imputed, interaction_lists, interaction_pairs, \"df with imputed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add logarithms and interactions to the predictors list\n",
    "predictors = predictors + log_cols + quad_cols + interaction_cols\n",
    "\n",
    "for predictor in remove_original_features:\n",
    "    if predictor in predictors:\n",
    "        predictors.remove(predictor)\n",
    "    \n",
    "if (gender is not None) and (\"gender.0\") in predictors:\n",
    "    predictors.remove(\"gender.0\") \n",
    "    \n",
    "    \n",
    "print(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions which runs lasso, build an ntermediate linear model by on features selected by lasso and \n",
    "# then build the final model on features significant by the inetrmediate model\n",
    "def model_builder(df_source, predictors, outcome_var, result_dir, keep_cholesterols, hcd, run_lasso = True):\n",
    "    \n",
    "    if not os.path.exists(result_dir):\n",
    "    # The directory does not exist, create it\n",
    "        os.makedirs(result_dir)\n",
    "        \n",
    "    y = df_source[outcome_var]\n",
    "    print(f\"outcome: {y}\")\n",
    "    \n",
    "    X = df_source[predictors]\n",
    "   \n",
    "    \n",
    "    ############ LASSO ###### \n",
    "    if (run_lasso):\n",
    "        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        # Initialize LassoCV with 10-fold cross-validation\n",
    "        lasso_cv = LassoCV(cv=100, random_state=42, max_iter=10000)\n",
    "        pipeline = make_pipeline(scaler, lasso_cv)\n",
    "        pipeline.fit(X, y)\n",
    "    \n",
    "    \n",
    "\n",
    "        summary_text = f\"Best alpha (regularization strength): {lasso_cv.alpha_}\\n\"\n",
    "        summary_text += f\"Intercept {lasso_cv.intercept_}\\nCoefficients:\\n\"\n",
    "        for idx, coef in enumerate(lasso_cv.coef_, start=1):\n",
    "            summary_text += f\"Feature {lasso_cv.coef_[idx]}: {coef}\\n\"\n",
    "\n",
    "        \n",
    "        # Saving to a file\n",
    "        with open(f\"{result_dir}/lasso_summary.txt\", 'w') as lasso_file:\n",
    "            lasso_file.write(summary_text)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        corrected_predictors = []\n",
    "        for i in range(len(predictors)):\n",
    "            if abs(lasso_cv.coef_[i]) >lasso_epsilon: # or (predictors[i] in simple_predictors):\n",
    "                print(f\"{predictors[i]}: {lasso_cv.coef_[i]}\")\n",
    "                corrected_predictors = corrected_predictors + [predictors[i]]\n",
    "        \n",
    "        if keep_cholesterols:\n",
    "            if \"ldl.0\" in corrected_predictors and not (\"hdl.0\" in corrected_predictors):\n",
    "                corrected_predictors = corrected_predictors  + [\"hdl.0\"]\n",
    "            if \"hdl.0\" in corrected_predictors and not (\"ldl.0\" in corrected_predictors):\n",
    "                corrected_predictors = corrected_predictors  + [\"ldl.0\"]  \n",
    "            \n",
    "   \n",
    "        if len(corrected_predictors)== 0:\n",
    "            print(\"All the coefficients are zeros\")\n",
    "            return None \n",
    "        \n",
    "        significant_features = corrected_predictors\n",
    "        ############ LINEAR MODEL AFTER LASSO ######\n",
    "        \n",
    "        '''\n",
    "        X_corr = df_source[corrected_predictors]\n",
    "        for clmn in  X_corr.columns:  \n",
    "            X_corr[clmn] = (X_corr[clmn] - X_corr[clmn].mean())/X_corr[clmn].std()\n",
    "    \n",
    "        X_corr = sm.add_constant(X_corr)\n",
    "        if hcd is not None:\n",
    "            model = sm.OLS(y, X_corr).fit(cov_type=hcd)\n",
    "        else:\n",
    "            model = sm.OLS(y, X_corr).fit()\n",
    "        \n",
    "        \n",
    "        with open(f\"{result_dir}/after_lasso_model_summary.txt\", 'w') as model_file:\n",
    "        # Convert the summary to a string and write it to the file\n",
    "            model_file.write(model.summary().as_text())\n",
    "        \n",
    "        ############ FINAL LINEAR MODEL  ###### \n",
    "    \n",
    "   \n",
    "        significant_features = model.pvalues[model.pvalues <= p_val].index.tolist()\n",
    "        \n",
    "        if 'const' in significant_features:\n",
    "            significant_features.remove('const') '''\n",
    "    else:\n",
    "       significant_features = predictors\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    X_corr_1 = df_source[significant_features]\n",
    "    df_mean_std = pd.DataFrame(columns = [\"predictor\", \"mean\", \"standard_deviation\", \"min\", \"max\"] )\n",
    "    i = 0\n",
    "    for clmn in  X_corr_1.columns:  \n",
    "        mean_ = X_corr_1[clmn].mean()\n",
    "        std_ = X_corr_1[clmn].std()\n",
    "        df_mean_std.loc[i] = [clmn, mean_, std_, X_corr_1[clmn].min(), X_corr_1[clmn].max()]\n",
    "        i += 1\n",
    "        X_corr_1[clmn] = (X_corr_1[clmn] - mean_)/std_\n",
    "        \n",
    "    \n",
    "    df_mean_std.to_csv(f\"{result_dir}/final_predictors_stat.csv\", sep=',')\n",
    "    \n",
    "        \n",
    "    X_corr_1 = sm.add_constant(X_corr_1) \n",
    "    if hcd is not None:\n",
    "        model_1= sm.OLS(y, X_corr_1).fit(cov_type=hcd) \n",
    "    else:\n",
    "        model_1= sm.OLS(y, X_corr_1).fit() \n",
    "    \n",
    "    \n",
    "    with open(f\"{result_dir}/final_model_summary.txt\", 'w') as model_1_file:\n",
    "    # Convert the summary to a string and write it to the file\n",
    "        model_1_file.write(model_1.summary().as_text())\n",
    "    \n",
    "    coefficients = model_1.params\n",
    "    p_values = model_1.pvalues\n",
    "    \n",
    "    df_results = pd.DataFrame({'Coefficient': coefficients, 'P_value': p_values})\n",
    "    \n",
    "    df_results = df_results.reset_index().rename(columns={df_results.reset_index().columns[0]: 'predictor'})\n",
    "    df_results = pd.merge(df_results, df_mean_std, on = \"predictor\", how = 'outer')\n",
    "    \n",
    "    \n",
    "    df_results.to_csv(f\"{result_dir}/final_model.csv\", sep=',')\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum TMTa_v00 if TMTa_v00!=. & TMTa_v02!=.\n",
    "scalar TMTamean_v00 = r(mean)\n",
    "sum TMTa_v00 if TMTa_v00!=. & TMTa_v02!=.\n",
    "scalar TMTaSD_v00 = r(sd)\n",
    "generate zTMTa_v00 = (TMTa_v00-TMTamean_v00)/TMTaSD_v00 if TMTa_v00!=. & TMTa_v02!=.\n",
    "\n",
    "sum TMTa_v02 if TMTa_v00!=. & TMTa_v02!=.\n",
    "scalar TMTamean_v02 = r(mean)\n",
    "sum TMTa_v02 if TMTa_v00!=. & TMTa_v02!=.\n",
    "scalar TMTaSD_v02 = r(sd)\n",
    "generate zTMTa_v02 = (TMTa_v02-TMTamean_v00)/TMTaSD_v00 if TMTa_v00!=. & TMTa_v02!=.\n",
    "*--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filetr predictors so that all the predictors with test names mentioned are removed\n",
    "# came from the earlier experiemnts where the amount of tests performed prior visit 2 were taken into account\n",
    "# possibly an obsolete step by no but let us keep it\n",
    "predictors_global =  copy.deepcopy(predictors)\n",
    "for predictor in predictors:\n",
    "    if \"snap_game\" in predictor:\n",
    "        predictors_global.remove(predictor)\n",
    "    if \"pairs_matching\" in predictor:\n",
    "        predictors_global.remove(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dependencies(df_, model_df, predictors_to_show, dir):\n",
    "    \n",
    "    if model_df is None:\n",
    "        return\n",
    "    \n",
    "    if os.path.exists(dir):\n",
    "        shutil.rmtree(dir)\n",
    "    os.makedirs(dir)\n",
    "    \n",
    "    print(\"we are building graphs\")\n",
    "    \n",
    "    for predictor in predictors_to_show:\n",
    "        \n",
    "        \n",
    "        print(predictor)\n",
    "       \n",
    "        y = np.zeros(df_.shape[0])\n",
    "        # \n",
    "        # aggregate all the terms containing pedictor \n",
    "        for feat in model_df['predictor']:\n",
    "            if not predictor in feat:\n",
    "                continue\n",
    "            if not feat in df_.columns:\n",
    "                continue\n",
    "            print(feat)\n",
    "            \n",
    "            # prepare the feature\n",
    "            match feat:\n",
    "                case _ if feat == predictor:\n",
    "                    fun_pred = df_[predictor]\n",
    "                case _ if feat == f\"log_{predictor}\":\n",
    "                    fun_pred = np.log1p(df_[predictor])\n",
    "                case _ if feat == f\"quadr_{predictor}\":\n",
    "                    fun_pred = np.square(df_[predictor])\n",
    "                case _ if feat == f\"quadr_log_{predictor}\":\n",
    "                    fun_pred = np.square(np.log1p(df_[predictor]))\n",
    "                case _:\n",
    "                    fun_pred = None\n",
    "                    print(f\"error, non existing predictor {feat}\")\n",
    "            # standartise the feature\n",
    "            fun_pred = (fun_pred - fun_pred.mean())/fun_pred.std()\n",
    "            \n",
    "            \n",
    "            \n",
    "            # find the coefficient\n",
    "            row_feat = model_df[model_df['predictor'] == feat].iloc[0]\n",
    "            coeff =  row_feat[\"Coefficient\"]\n",
    "            \n",
    "            y = y + coeff *fun_pred\n",
    "        \n",
    "        # x_pred = (df_[predictor]    - df_[predictor].mean()) /  df_[predictor].std()\n",
    "        # x = np.linspace(min(x_pred), max(x_pred), 100)  \n",
    "          \n",
    "        are_all_zeros = np.all(y == 0)        \n",
    "        if not are_all_zeros:\n",
    "            x = df_[predictor]\n",
    "        \n",
    "            plt.scatter(x, y)\n",
    "            plt.xlabel(predictor)\n",
    "            plt.ylabel('outcome')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "            # Save the plot as a PNG file\n",
    "            plt.savefig(f\"{dir}/{predictor}.png\", format='png', dpi=300)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outcomes = [f\"z_global.{second_visit}\", f\"z_change_{memory_experiment_full_name}.0.{second_visit}\",  f\"z_change_snap_game_true_pos_rt_avrg.0.{second_visit}\"]\n",
    "model_table_no_na = {}\n",
    "model_table_imputed = {}\n",
    "model_table_no_na_simple = {}\n",
    "model_table_imputed_simple = {}\n",
    "for outcome in outcomes:\n",
    "    print(outcome)\n",
    "    \n",
    "    # main\n",
    "    \n",
    "    print('main no na')\n",
    "    model_table_no_na[outcome] = model_builder(df, predictors_global, outcome, f\"{output_dir}/no_na_{outcome}\", keep_ldl_hdl, HSCD)\n",
    "    show_dependencies(df, model_table_no_na[outcome], predictors_to_plot, f\"{output_dir}/no_na_{outcome}/figures\")\n",
    "    print('main imputed')\n",
    "    model_table_imputed[outcome] = model_builder(df_imputed, predictors_global, outcome, f\"{output_dir}/imputed_{outcome}\", keep_ldl_hdl, HSCD)\n",
    "    show_dependencies(df_imputed, model_table_imputed[outcome], predictors_to_plot, f\"{output_dir}/imputed_{outcome}/figures\")\n",
    "    \n",
    "    if compute_simple_models:\n",
    "        # simple\n",
    "        \n",
    "        print('simple no na')\n",
    "        model_table_no_na_simple[outcome] = model_builder(df, simple_predictors, outcome, f\"{output_dir}/simple_no_na_{outcome}\", keep_ldl_hdl, HSCD, False)\n",
    "        \n",
    "        print('simple imputed')\n",
    "        model_table_imputed_simple[outcome] = model_builder(df_imputed, simple_predictors, outcome, f\"{output_dir}/simple_imputed_{outcome}\", keep_ldl_hdl, HSCD, False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copula",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
