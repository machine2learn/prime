{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "matplotlib.use('module://ipykernel.pylab.backend_inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import statsmodels.api as sm\n",
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.use('module://ipykernel.pylab.backend_inline')\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_difference(df_source, i):\n",
    "    assignment = {}\n",
    "    assignment[f\"time_difference.0.{i}\"] = (pd.to_datetime(df_source[f\"date_visit.{i}\"]) - pd.to_datetime(df_source[\"date_visit.0\"])).dt.total_seconds() / (24 * 3600)\n",
    "    df_source = df_source.assign(**assignment)\n",
    "    return df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add log features \n",
    "def add_logs(df_, log_features, outcome_components, descr):\n",
    "    print(f\"adding logarithmic features to {descr}\")\n",
    "    assignment = {}\n",
    "    log_cols = []\n",
    "\n",
    "    for log_feature in log_features:\n",
    "        if log_feature in df_.columns:\n",
    "            new_fld = f\"log_{log_feature}\"\n",
    "            assignment[new_fld] = np.log1p(df_[log_feature])\n",
    "            log_cols = log_cols + [new_fld]\n",
    "            print(new_fld)\n",
    "    \n",
    "    for i in range(0,4):\n",
    "        \n",
    "        if f\"pairs_matching_sum_incorrect.{i}\" in outcome_components:\n",
    "            new_fld = f\"log_pairs_matching_sum_incorrect.{i}\"\n",
    "            if new_fld not in assignment:\n",
    "                assignment[new_fld] = np.log1p(df_[f\"pairs_matching_sum_incorrect.{i}\"])\n",
    "                print(new_fld)\n",
    "                \n",
    "        \n",
    "        if f\"pairs_matching_incorrect_in_round_1.{i}\" in outcome_components:\n",
    "            new_fld_round = f\"log_pairs_matching_incorrect_in_round_1.{i}\"\n",
    "            if new_fld_round not in assignment:\n",
    "                assignment[new_fld_round] = np.log1p(df_[f\"pairs_matching_incorrect_in_round_1.{i}\"])  \n",
    "                print(new_fld) \n",
    "\n",
    "    df_ = df_.assign(**assignment)\n",
    "    return df_, log_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to quadr features \n",
    "def add_squares(df_, q_features, descr, skewed_only_on_log, skewed_features):\n",
    "    print(f\"adding quadratic features to {descr}\")\n",
    "    assignment = {}\n",
    "    quadr_cols = []\n",
    "    \n",
    "    for feat in df_.columns:\n",
    "        for q_feat in q_features:\n",
    "            if q_feat in feat:\n",
    "                if not skewed_only_on_log or (\"log_\" in feat) or not (feat in skewed_features):\n",
    "                    new_fld = f\"quadr_{feat}\"\n",
    "                    assignment[new_fld] = np.square(df_[feat])\n",
    "                    quadr_cols = quadr_cols + [new_fld]\n",
    "                    print(new_fld)\n",
    "    \n",
    "    \n",
    "\n",
    "    df_ = df_.assign(**assignment)\n",
    "    return df_, quadr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for removing outliers via iqr approach\n",
    "def remove_outliers_iqr(df_, iqr_cf, df_name, outliers, log_feat):\n",
    "    if iqr_cf is not None:\n",
    "        \n",
    "        lower_bound = {}\n",
    "        upper_bound = {}\n",
    "        \n",
    "        for feat  in outliers:\n",
    "            print(f\"{feat}\")\n",
    "            \n",
    "            Q1 = df_[feat].quantile(0.25)\n",
    "            Q3 = df_[feat].quantile(0.75)\n",
    "            \n",
    "            IQR = Q3-Q1\n",
    "            lower_bound[feat] = Q1 - iqr_cf * IQR\n",
    "            upper_bound[feat] = Q3 + iqr_cf * IQR\n",
    "            \n",
    "            if feat in log_feat:\n",
    "                feat=f\"log_{feat}\"\n",
    "                print(f\"logarithimic outlier {feat}\")\n",
    "            \n",
    "                Q1 = df_[feat].quantile(0.25)\n",
    "                Q3 = df_[feat].quantile(0.75)\n",
    "                \n",
    "                IQR = Q3-Q1\n",
    "                lower_bound[feat] = Q1 - iqr_cf * IQR\n",
    "                upper_bound[feat] = Q3 + iqr_cf * IQR\n",
    "            \n",
    "            \n",
    "                \n",
    "         \n",
    "            \n",
    "        for feat in lower_bound:\n",
    "            df_= df_[(df_[feat] >= lower_bound[feat]) & (df_[feat] <= upper_bound[feat])]\n",
    "            \n",
    "        print(f\"data set {df_name} after removing outliers: {len(df_)}\")\n",
    "    return(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which adds the column with the RV scores to the data set\n",
    "def adding_rv_scores(df_source, mem_experiment_full, repeat):\n",
    "    assignment = {}\n",
    "    weight_mem = 1\n",
    "    if \"pairs_matching\" in mem_experiment_full:\n",
    "        weight_mem = -1\n",
    "        \n",
    "    sum_mem_0 = df_source[f\"{mem_experiment_full}.0\"].sum()\n",
    "    mem_mean_0 = df_source[f\"{mem_experiment_full}.0\"].mean()\n",
    "    mem_std_0 = df_source[f\"{mem_experiment_full}.0\"].std()\n",
    "    assignment[f'z_{mem_experiment_full}.0'] = (df_source[f\"{mem_experiment_full}.0\"] - mem_mean_0)/mem_std_0\n",
    "    \n",
    "    sum_mem_repeat = df_source[f\"{mem_experiment_full}.{repeat}\"].sum()\n",
    "    mem_mean_repeat = df_source[f\"{mem_experiment_full}.{repeat}\"].mean()\n",
    "    mem_std_repeat = df_source[f\"{mem_experiment_full}.{repeat}\"].std()\n",
    "    assignment[f'z_{mem_experiment_full}.{repeat}'] = (df_source[f\"{mem_experiment_full}.{repeat}\"] - mem_mean_0)/mem_std_0 \n",
    "    \n",
    "    sum_sg_rt_mean_0 = df_source[\"snap_game_true_pos_rt_avrg.0\"].sum()\n",
    "    sg_rt_mean_0 = df_source[\"snap_game_true_pos_rt_avrg.0\"].mean()\n",
    "    sg_rt_std_0 = df_source[\"snap_game_true_pos_rt_avrg.0\"].std()\n",
    "    assignment['z_snap_game_true_pos_rt_avrg.0'] = (df_source[\"snap_game_true_pos_rt_avrg.0\"] - sg_rt_mean_0)/sg_rt_std_0\n",
    "\n",
    "    sum_sg_rt_mean_repeat = df_source[f\"snap_game_true_pos_rt_avrg.{repeat}\"].sum()\n",
    "    sg_rt_mean_repeat = df_source[f\"snap_game_true_pos_rt_avrg.{repeat}\"].mean() \n",
    "    sg_rt_std_repeat = df_source[f\"snap_game_true_pos_rt_avrg.{repeat}\"].std()\n",
    "    assignment[f'z_snap_game_true_pos_rt_avrg.{repeat}'] = (df_source[f\"snap_game_true_pos_rt_avrg.{repeat}\"] - sg_rt_mean_0)/sg_rt_std_0 \n",
    "    \n",
    "     # generate Global_v00 = (zDSTf_v00 + zDSTb_v00 - zTMTa_v00 - zTMTb_v00) / 4\n",
    "    assignment[\"global.0\"] = (- assignment['z_snap_game_true_pos_rt_avrg.0'] + weight_mem * assignment[f'z_{mem_experiment_full}.0'])/2.0\n",
    "    sum_global_0 = assignment[\"global.0\"].sum()\n",
    "    mean_global_0 = assignment[\"global.0\"].mean()\n",
    "    std_global_0 = assignment[\"global.0\"].std()\n",
    "    assignment['z_global.0'] = (assignment[\"global.0\"] - mean_global_0)/std_global_0\n",
    "\n",
    "    assignment[f\"global.{repeat}\"] = (- assignment[f'z_snap_game_true_pos_rt_avrg.{repeat}'] + weight_mem * assignment[f'z_{mem_experiment_full}.{repeat}'])/2.0 \n",
    "    assignment[f\"z_global.{repeat}\"] = (assignment[f\"global.{repeat}\"] - mean_global_0)/std_global_0\n",
    "    \n",
    "    assignment[f\"z_change_{mem_experiment_full}.0.{repeat}\"] = assignment[f'z_{mem_experiment_full}.{repeat}'] - assignment[f'z_{mem_experiment_full}.0']\n",
    "    assignment[f\"z_change_snap_game_true_pos_rt_avrg.0.{repeat}\"] = assignment[f'z_snap_game_true_pos_rt_avrg.{repeat}'] - assignment['z_snap_game_true_pos_rt_avrg.0']\n",
    "    df_source = df_source.assign(**assignment) \n",
    "    return df_source  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_median_score(df_source, predictors, score_clmn, n_steps):\n",
    "    \n",
    "    assignment = {}\n",
    "    assignment[f\"median_{score_clmn}\"] = None\n",
    "    df_source = df_source.assign(**assignment)\n",
    "    \n",
    "    step = {}\n",
    "    for clmn in predictors:\n",
    "        \n",
    "        step[clmn] = (df_source[clmn].max() - df_source[clmn].min())/n_steps\n",
    "        \n",
    "    for i in range(len(df_source)):\n",
    "        print(f\"participant {i}\")\n",
    "        df_filter = copy.deepcopy(df_source)\n",
    "        \n",
    "        for clmn in predictors:\n",
    "           \n",
    "            left = df_source.at[i, clmn] - step[clmn]\n",
    "            right = df_source.at[i, clmn] + step[clmn]\n",
    "            df_filter = df_filter[df_filter[clmn].ge(left) & df_filter[clmn].le(right)]\n",
    "        \n",
    "        current_median = df_filter[score_clmn].median()\n",
    "       \n",
    "        df_source.at[i, f\"median_{score_clmn}\"] = current_median\n",
    "    \n",
    "    return df_source\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples =None\n",
    "n_steps_for_median = 10\n",
    "\n",
    "memory_experiment = \"pm\"\n",
    "\n",
    "binary_smoke = False\n",
    "\n",
    "second_visit = 2\n",
    "age_limit = 0\n",
    "gender = None\n",
    "iqr_coefficient = None # if None then no standrat removal of outliers\n",
    "\n",
    "keep_ldl_hdl = True # if lasso selects only one of them keep the second anyway; they are correlated but have different influence\n",
    "quadr_on_log_if_skewed = True\n",
    "\n",
    "input_dir = \"/projects/prime/ukbb/preprocessed_data_2024\"\n",
    "imputed_dir = f\"/projects/prime/ukbb/preprocessed_data_2024/imputations/ukbb_imputed_{memory_experiment}_sg_0_{second_visit}_rf/ukbb_imputed_{memory_experiment}_sg_0_{second_visit}_rf_instance_1\"\n",
    "path_imputed =  f\"{imputed_dir}/ukbb_imputed_{memory_experiment}_sg_0_{second_visit}_rf_instance_1.csv\"\n",
    "input_filenames = {\n",
    "    \"diet\": \"diet_2024_bradbury_march.csv\",\n",
    "    \"alcohol\": \"alcohol_2024_maart.csv\",\n",
    "    \"diagnoses\": \"diagnoses_2024.csv\",\n",
    "    \"lifestyle\": \"participants_age_date_smoke_MET_2024_jan.csv\",\n",
    "    \"medications\": \"participants_medications_2024_jan.csv\",\n",
    "    \"risk_factors\": \"blood_count_biochemistry_pressure_maart_2024.csv\",\n",
    "    \"sociodemo\": \"edu_job_marriage_2024.csv\",\n",
    "    \"cognition\": \"cognitive_2024_maart.csv\",\n",
    "    \"metrics\": \"body_size_measures_gender_dates_2024_jan.csv\"\n",
    "  }\n",
    "\n",
    "output_dir =f\"/projects/prime/ukbb/results_2024/{memory_experiment}_sg_0_{second_visit}_median_05_06_all\"\n",
    "\n",
    "if gender is None:\n",
    "  output_dir = f\"{output_dir}/\"\n",
    "else:\n",
    "  output_dir = f\"{output_dir}_gender_{gender}/\"\n",
    "if not os.path.exists(output_dir):\n",
    "  os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_data_file = f\"{output_dir}/data_non_na.csv\"\n",
    "out_imputed_data_file = f\"{output_dir}/data_ukbb_imputed_{memory_experiment}_sg_0_{second_visit}_rf_instance_1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if memory_experiment == \"nm\":\n",
    "    memory_experiment_full_name = \"num_memory_max_digits_remembered_correctly\"\n",
    "else:\n",
    "    memory_experiment_full_name = \"pairs_matching_sum_incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "predictors = [\"gender.0\", f\"time_difference.0.{second_visit}\", \"Diabetes_2.0\",  \"Dyslipidemia.0\", \"Depression.0\", \"Hypertension.0\", \n",
    "  \"age_years.0\", \"education_level.0\", \"marital_status.0\", \"pp_smoke_catgeory.0\", \"MET_score.0\", \"waist_cm.0\",  \"bmi_kg_m2.0\", \n",
    "  \"Syst_bp.0\", \"Diast_bp.0\", \"ldl_conv.0\", \"hdl_conv.0\", \"Triglycerides_conv.0\", \"HbA1c_conv.0\",  \"Pl_glucose_conv.0\",   \"Albumin.0\", \n",
    "  \"Leukocyte_count.0\", \"C_reactive_protein.0\",\n",
    "   \"oily_fish_gpday_bradbury.0\", \"white_fish_gpday_bradbury.0\", \n",
    "   \"red_meat_bradbury_gpd.0\", \"poultry_gpday_bradbury.0\",\n",
    "   \"processed_meat_gpday_bradbury.0\", \"veg_gpday_bradbury.0\",\n",
    "   \"fruit_gpday_bradbury.0\",\n",
    "    \"cereals_gpday_bradbury.0\", \"bread_gpday_bradbury.0\", \"cheese_gpday_bradbury.0\",\n",
    "    \"milk_gpday_bradbury.0\", \"tea_gpday_bradbury.0\",\n",
    "    \"red_wine_gpd.0\", \"white_wine_gpd.0\", \"fortified_gpd.0\", \"beer_cider_gpd.0\", \"spirits_gpd.0\",\n",
    "   \"aspirin.0\",  \"anxiety_tr.0\", \"pain_tr.0\", \"TAZD_Thiazide.0\",  \n",
    "   \"loop_diuretics.0\",  \"potassium_diuretics.0\", \"beta_blockers.0\", \"calcium_antagonists.0\", \n",
    "   \"ARA_II_Antagonists_of_angiotensin_II_receptors.0\", \"IECA_Angiotensin_converting_enzyme_inhibitors.0\",\n",
    "   \"Other_Hypotensive.0\",\"hypochol_statins.0\", \"hypochol_others.0\", \"insulin.0\", \"sulfonylurea.0\", \"thiazolidinediones.0\", \"non_sulfonylurea_insulin_secretagogues.0\", \"metformin_category.0\", \"vitamins_minerals.0\"]\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [\"gender.0\", \"age_years.0\", \"education_level.0\", \n",
    "   f\"time_difference.0.{second_visit}\",\n",
    "   \"oily_fish_gpday_bradbury.0\", \"white_fish_gpday_bradbury.0\", \n",
    "   \"red_meat_bradbury_gpd.0\", \"poultry_gpday_bradbury.0\",\n",
    "   \"processed_meat_gpday_bradbury.0\", \"veg_gpday_bradbury.0\",\n",
    "   \"fruit_gpday_bradbury.0\",\n",
    "    \"cereals_gpday_bradbury.0\", \"bread_gpday_bradbury.0\", \"cheese_gpday_bradbury.0\",\n",
    "    \"milk_gpday_bradbury.0\", \"tea_gpday_bradbury.0\",\n",
    "    \"red_wine_gpd.0\", \"white_wine_gpd.0\", \"fortified_gpd.0\", \"beer_cider_gpd.0\", \"spirits_gpd.0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_to_plot = [f\"time_difference.0.{second_visit}\", \n",
    "  \"age_years.0\", \"MET_score.0\", \"waist_cm.0\",  \"bmi_kg_m2.0\", \n",
    "  \"Syst_bp.0\", \"Diast_bp.0\", \"ldl_conv.0\", \"hdl_conv.0\", \"Triglycerides_conv.0\", \"HbA1c_conv.0\",  \"Pl_glucose_conv.0\",   \"Albumin.0\", \n",
    "  \"Leukocyte_count.0\", \"C_reactive_protein.0\",\n",
    "   \"oily_fish_gpday_bradbury.0\", \"white_fish_gpday_bradbury.0\", \n",
    "   \"red_meat_bradbury_gpd.0\", \"poultry_gpday_bradbury.0\",\n",
    "   \"processed_meat_gpday_bradbury.0\", \"veg_gpday_bradbury.0\",\n",
    "   \"fruit_gpday_bradbury.0\",\n",
    "    \"cereals_gpday_bradbury.0\", \"bread_gpday_bradbury.0\", \"cheese_gpday_bradbury.0\",\n",
    "    \"milk_gpday_bradbury.0\", \"tea_gpday_bradbury.0\",\n",
    "    \"red_wine_gpd.0\", \"white_wine_gpd.0\", \"fortified_gpd.0\", \"beer_cider_gpd.0\", \"spirits_gpd.0\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_standart = [\"Syst_bp.0\", \"Diast_bp.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_predictors =[\"age_years.0\", \"gender.0\", \"education_level.0\", f\"log_time_difference.0.{second_visit}\"]\n",
    "if (gender is not None) and (\"gender.0\") in simple_predictors:\n",
    "    simple_predictors.remove(\"gender.0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_components = [f\"{memory_experiment_full_name}.0\", f\"{memory_experiment_full_name}.{second_visit}\", \n",
    "            \"snap_game_true_pos_rt_avrg.0\", f\"snap_game_true_pos_rt_avrg.{second_visit}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_features = [f\"time_difference.0.{second_visit}\", \"age_years.0\", \"MET_score.0\", \n",
    "  \"Triglycerides_conv.0\", \"HbA1c_conv.0\",  \"Pl_glucose_conv.0\",   \n",
    "  \"Leukocyte_count.0\", \"C_reactive_protein.0\",\n",
    "  \"red_wine_gpd.0\", \"white_wine_gpd.0\", \"fortified_gpd.0\", \"beer_cider_gpd.0\", \"spirits_gpd.0\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadr_features = [\"MET_score.0\", \"waist_cm.0\",  \"bmi_kg_m2.0\", \n",
    "  \"Syst_bp.0\", \"Diast_bp.0\", \"ldl_conv.0\", \"hdl_conv.0\", \"Triglycerides_conv.0\", \"HbA1c_conv.0\",  \"Pl_glucose_conv.0\",   \"Albumin.0\", \n",
    "  \"Leukocyte_count.0\", \"C_reactive_protein.0\",\n",
    "   \"oily_fish_gpday_bradbury.0\", \"white_fish_gpday_bradbury.0\", \n",
    "   \"red_meat_bradbury_gpd.0\", \"poultry_gpday_bradbury.0\",\n",
    "   \"processed_meat_gpday_bradbury.0\", \"veg_gpday_bradbury.0\",\n",
    "   \"fruit_gpday_bradbury.0\",\n",
    "    \"cereals_gpday_bradbury.0\", \"bread_gpday_bradbury.0\", \"cheese_gpday_bradbury.0\",\n",
    "    \"milk_gpday_bradbury.0\", \"tea_gpday_bradbury.0\"\n",
    "    \"red_wine_gpd.0\", \"white_wine_gpd.0\", \"fortified_gpd.0\", \"beer_cider_gpd.0\", \"spirits_gpd.0\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''outliers = {\n",
    "    \"ldl_conv.0\": [0, 290],\n",
    "    \"Syst_bp.0\": [0, 220],\n",
    "}'''\n",
    "outliers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# very skewed:\n",
    "remove_original_features = [\"MET_score.0\", \"Triglycerides_conv.0\", \"Leukocyte_count.0\", \"C_reactive_protein.0\"\n",
    "                            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_needed = [\"f.eid\"] + predictors + outcome_components\n",
    "for i in range(1,4):\n",
    "    if f\"time_difference.0.{i}\" in predictors: # will be calculated from dates\n",
    "        all_needed.remove(f\"time_difference.0.{i}\")\n",
    "        all_needed = all_needed + [f\"date_visit.{i}\"] \n",
    "        if \"date_visit.0\" not in all_needed:\n",
    "             all_needed = all_needed + [\"date_visit.0\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data set and select all not NA\n",
    "if not os.path.exists(out_data_file):\n",
    "    df_index = {}\n",
    "    first = True\n",
    "    for label, filename in input_filenames.items():\n",
    "        print(label)\n",
    "        full_path = f\"{input_dir}/{filename}\"\n",
    "        \n",
    "        data_columns = pd.read_table(full_path, nrows=1, sep=',').columns\n",
    "        \n",
    "        needed_fields = data_columns[data_columns.isin(all_needed)]\n",
    "        \n",
    "        if label == \"medications\":\n",
    "            needed_fields = needed_fields.tolist() + [\"alpha_glucosidase_inhibitors.0\"] \n",
    "            \n",
    "        df_index[label] = pd.read_table(full_path, sep=',', usecols=needed_fields, dtype=str, nrows=n_samples)\n",
    "        \n",
    "        \n",
    "        print(len(df_index[label]))\n",
    "        \n",
    "        for clmn in needed_fields:\n",
    "            if clmn == \"f.eid\" or clmn.startswith(\"date_visit\"):\n",
    "                continue\n",
    "            df_index[label][clmn] = df_index[label][clmn].astype(float)\n",
    "        \n",
    "    \n",
    "        if first:\n",
    "            df = df_index[label]\n",
    "            first = False\n",
    "        else:\n",
    "            df = pd.merge(df,df_index[label])\n",
    "            \n",
    "        print(len(df))\n",
    "\n",
    "\n",
    "\n",
    "    # remove NA from the check up data set \n",
    "    df = df.dropna(axis=\"rows\")\n",
    "    print(f\"cleaned check up db has {len(df)} rows\")\n",
    "\n",
    "    df= df.loc[df[\"alpha_glucosidase_inhibitors.0\"] != 1]\n",
    "    print(\"removed all participants with alpha_glucosidase_inhibitors.0 == 1:\")  \n",
    "    df = df.drop(\"alpha_glucosidase_inhibitors.0\", axis = 1)\n",
    "    print(len(df))\n",
    "    \n",
    "    # filter both data set by age limits (which can be zero)\n",
    "    if \"age_years.0\" in df.columns:\n",
    "        df= df[df[\"age_years.0\"].ge(age_limit)]\n",
    "        print(\"removed all participants younger age_limits no NA:\") \n",
    "        print(len(df))\n",
    "\n",
    "        # filter both data sets by gender if required\n",
    "    if gender is not None and \"gender.0\" in df.columns:\n",
    "        df= df.loc[df[\"gender.0\"].eq(gender)]\n",
    "        df.drop(columns=['gender.0'])\n",
    "        print(f\"only participants with gender {gender} are left in the not na data set: \") \n",
    "        print(len(df)) \n",
    "    \n",
    "    # add to both data data sets time difference column\n",
    "    for i in range(1,4):\n",
    "        if f\"time_difference.0.{i}\" in predictors:\n",
    "            df = add_time_difference(df, i)\n",
    "    \n",
    "    if binary_smoke:\n",
    "    \n",
    "        ind_replace = predictors.index(\"smoking_score.0\")\n",
    "        predictors[ind_replace] = \"binary_smoking_score.0\"\n",
    "        \n",
    "        assignment = {}\n",
    "        assignment[\"binary_smoking_score.0\"] =np.where(df[\"smoking_score.0\"] == 2,1, 0) \n",
    "        df = df.assign(**assignment)\n",
    "        \n",
    "    # remove from both data sets customized outliers\n",
    "    for feat, out_ in outliers.items():\n",
    "        df= df[df[feat] >= out_[0] & df[feat] <= out_[1]]\n",
    "        print(f\"data set no na after removing customized outliers: {len(df)}\")\n",
    "\n",
    "    df, log_cols =  add_logs(df, log_features, outcome_components, \"check up db with no NA\")\n",
    "    df, quad_cols =  add_squares(df, quadr_features, \"check up db with no NA\", quadr_on_log_if_skewed, remove_original_features)\n",
    "    \n",
    "    \n",
    "    if iqr_coefficient is not None:\n",
    "        df = remove_outliers_iqr(df, iqr_coefficient, \"no na\")\n",
    "    \n",
    "    if memory_experiment == \"pm\":\n",
    "        memory_experiment = f\"log_{memory_experiment}\"\n",
    "        memory_experiment_full_name = f\"log_{memory_experiment_full_name}\"\n",
    "\n",
    "    df = adding_rv_scores(df, memory_experiment_full_name, second_visit)\n",
    "    ## ADD MEDIANS\n",
    "    outcomes = [f\"z_global.{second_visit}\", f\"z_change_{memory_experiment_full_name}.0.{second_visit}\",  f\"z_change_snap_game_true_pos_rt_avrg.0.{second_visit}\"]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    if n_steps_for_median is not None:\n",
    "        for outcome in outcomes:\n",
    "            df = add_median_score(df, predictors, outcome, n_steps_for_median)\n",
    "    df.to_csv(out_data_file, sep=',')\n",
    "else:\n",
    "    df= pd.read_table(out_data_file, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load imputed data set\n",
    "if not os.path.exists(out_imputed_data_file):\n",
    "    df_imputed = pd.read_table(path_imputed, sep=',', usecols=all_needed, dtype=str, nrows=n_samples)\n",
    "    # make all columns if the imputed except dates, float\n",
    "    for clmn in df_imputed.columns:\n",
    "        if clmn == \"f.eid\" or clmn.startswith(\"date_visit\"):\n",
    "            continue\n",
    "        df_imputed[clmn] = df_imputed[clmn].astype(float)\n",
    "        \n",
    "    if \"age_years.0\" in df_imputed.columns:\n",
    "        df_imputed= df_imputed[df_imputed[\"age_years.0\"].ge(age_limit)]\n",
    "        print(\"removed all participants younger age_limits imputed:\") \n",
    "        print(len(df_imputed))\n",
    "        \n",
    "        # filter both data sets by gender if required\n",
    "    if gender is not None and \"gender.0\" in df_imputed.columns:\n",
    "        df_imputed= df_imputed.loc[df_imputed[\"gender.0\"].eq(gender)]\n",
    "        df_imputed.drop(columns=['gender.0'])\n",
    "        print(f\"only participants with gender {gender} are left in the omputed data set: \") \n",
    "        print(len(df_imputed))\n",
    "\n",
    "    # add to both data data sets time difference column\n",
    "    for i in range(1,4):\n",
    "        if f\"time_difference.0.{i}\" in predictors:\n",
    "            df_imputed = add_time_difference(df_imputed, i)\n",
    "    \n",
    "    if binary_smoke:\n",
    "    \n",
    "        if \"smoking_score.0\" in predictors:\n",
    "            ind_replace = predictors.index(\"smoking_score.0\")\n",
    "            predictors[ind_replace] = \"binary_smoking_score.0\"\n",
    "        \n",
    "        assignment = {}\n",
    "        assignment[\"binary_smoking_score.0\"] =np.where(df_imputed[\"smoking_score.0\"] == 2,1, 0) \n",
    "        df_imputed = df_imputed.assign(**assignment)\n",
    "        \n",
    "        # remove from both data sets customized outliers\n",
    "    for feat, out_ in outliers.items():\n",
    "        df_imputed= df_imputed[(df_imputed[feat] >= out_[0]) & (df_imputed[feat] <= out_[1])]\n",
    "        print(f\"data set imputed after removing customized outliers: {len(df_imputed)}\")\n",
    "    \n",
    "    df_imputed, _  =  add_logs(df_imputed, log_features, outcome_components,\"df with imputed\")      \n",
    "    df_imputed, _  =  add_squares(df_imputed, quadr_features, \"df with imputed\", quadr_on_log_if_skewed, remove_original_features)    \n",
    "    # removing outliers via iqr approach\n",
    "    \n",
    "    if iqr_coefficient is not None:\n",
    "        df_imputed = remove_outliers_iqr(df_imputed, iqr_coefficient, \"imputed\")\n",
    "    \n",
    "    if memory_experiment == \"pm\":\n",
    "        memory_experiment = f\"log_{memory_experiment}\"\n",
    "        memory_experiment_full_name = f\"log_{memory_experiment_full_name}\"\n",
    "        \n",
    "    df_imputed = adding_rv_scores(df_imputed, memory_experiment_full_name, second_visit)\n",
    "   ## ADD MEDIANS\n",
    "    outcomes = [f\"z_global.{second_visit}\", f\"z_change_{memory_experiment_full_name}.0.{second_visit}\",  f\"z_change_snap_game_true_pos_rt_avrg.0.{second_visit}\"]\n",
    "    df_imputed.reset_index(drop=True, inplace=True)\n",
    "    if n_steps_for_median is not None:\n",
    "        for outcome in outcomes:\n",
    "            df_imputed = add_median_score(df_imputed, predictors, outcome, n_steps_for_median)\n",
    "    df_imputed.to_csv(out_imputed_data_file, sep=',')\n",
    "else:\n",
    "    df_imputed = pd.read_table(out_imputed_data_file, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe population\n",
    "def describe_population(df_, descr): \n",
    "    print(descr)\n",
    "    comorbidity = [\"Diabetes_2.0\",  \"Dyslipidemia.0\", \"Depression.0\", \"Hypertension.0\"]\n",
    "    n_comor = {}\n",
    "    percentage_comor = {}\n",
    "    n_comor_female = {}\n",
    "    percentage_comor_female = {}\n",
    "    for comor in comorbidity:\n",
    "        if comor in df_.columns: \n",
    "            n_comor[comor] = df_[comor].sum()\n",
    "            percentage_comor[comor] = (float(n_comor[comor])*100)/float(len(df_))\n",
    "            print(f\"The % of participants with {comor} is {percentage_comor[comor]}\")\n",
    "            if \"gender.0\" in df_.columns:\n",
    "                n_comor_female[comor] = (df_[comor] + df_[\"gender.0\"]).eq(2.0).sum()\n",
    "                percentage_comor_female[comor] = (float(n_comor_female[comor])*100)/float(len(df_))\n",
    "                print(f\"The % of female participants with {comor} is {percentage_comor_female[comor]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copula",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
