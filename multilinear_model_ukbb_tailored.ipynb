{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "matplotlib.use('module://ipykernel.pylab.backend_inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "    \n",
    "\n",
    "import statsmodels.api as sm\n",
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.use('module://ipykernel.pylab.backend_inline')\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outcomes  = [f\"z_global.{second_visit}\", f\"z_change_{memory_experiment_full_name}.0.{second_visit}\",  f\"z_change_snap_game_true_pos_rt_avrg.0.{second_visit}\",\n",
    "             f\"median_z_global.{second_visit}\", f\"median_z_change_{memory_experiment_full_name}.0.{second_visit}\",  f\"median_z_change_snap_game_true_pos_rt_avrg.0.{second_visit}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_visit = 2\n",
    "predictors = [\"gender.0\", f\"log_time_difference.0.{second_visit}\", \n",
    "              \"Diabetes_2.0\",  \"Dyslipidemia.0\", \"Depression.0\", \"Hypertension.0\", \n",
    "  \"age_years.0\", \"education_level.0\", \"marital_status.0\", \"pp_smoke_catgeory.0\", \"log_MET_score.0\", \"waist_cm.0\",  \"log_bmi_kg_m2.0\", \n",
    "  \"Syst_bp.0\", \"Diast_bp.0\", \"log_ldl_conv.0\", \"log_hdl_conv.0\", \"log_Triglycerides_conv.0\", \"log_HbA1c_conv.0\",  \"log_Pl_glucose_conv.0\",   \"log_Albumin.0\", \n",
    "  \"log_Leukocyte_count.0\", \"log_C_reactive_protein.0\",\n",
    "   \"log_oily_fish_gpday_bradbury.0\", \"log_white_fish_gpday_bradbury.0\", \n",
    "   \"log_red_meat_bradbury_gpd.0\", \"log_poultry_gpday_bradbury.0\",\n",
    "   \"log_processed_meat_gpday_bradbury.0\", \"log_veg_gpday_bradbury.0\",\n",
    "   \"log_fruit_gpday_bradbury.0\",\n",
    "    \"log_cereals_gpday_bradbury.0\", \"log_bread_gpday_bradbury.0\", \"log_cheese_gpday_bradbury.0\",\n",
    "    \"log_milk_gpday_bradbury.0\", \"log_tea_gpday_bradbury.0\",\n",
    "    \"log_red_wine_gpd.0\", \"log_white_wine_gpd.0\", \"log_fortified_gpd.0\", \"log_beer_cider_gpd.0\", \"log_spirits_gpd.0\",\n",
    "   \"aspirin.0\",  \"anxiety_tr.0\", \"pain_tr.0\", \"TAZD_Thiazide.0\",  \n",
    "   \"loop_diuretics.0\",  \"potassium_diuretics.0\", \"beta_blockers.0\", \"calcium_antagonists.0\", \n",
    "   \"ARA_II_Antagonists_of_angiotensin_II_receptors.0\", \"IECA_Angiotensin_converting_enzyme_inhibitors.0\",\n",
    "   \"Other_Hypotensive.0\",\"hypochol_statins.0\", \"hypochol_others.0\", \"insulin.0\", \"sulfonylurea.0\", \"thiazolidinediones.0\", \"non_sulfonylurea_insulin_secretagogues.0\", \"metformin_category.0\", \"vitamins_minerals.0\"]\n",
    "\n",
    "n_samples = None\n",
    "\n",
    "memory_experiment = \"nm\"\n",
    "\n",
    "\n",
    "\n",
    "lasso_epsilon = 0.00001\n",
    "p_val = 0.05\n",
    "age_limit = 0\n",
    "gender = 0\n",
    "iqr_coefficient = 1.5 # if None then no standrat removal of outliers\n",
    "\n",
    "\n",
    "HSCD = 'HC1' # if none than the model builder will be heteroscedascisity unaware\n",
    "remove_na = True\n",
    "\n",
    "\n",
    "working_dir =f\"/projects/prime/ukbb/results_2024/{memory_experiment}_sg_0_{second_visit}_19_06_all_learning/\"\n",
    "# help_model_file_second_run = f\"{working_dir}/non_na_median_z_global.2_HSCD_HC1_firs_run_lasso_0.01/final_model.csv\"\n",
    "# help_model_file_second_run = None\n",
    "# help_model_file_second_run = f\"{working_dir}/non_na_z_global.2_HSCD_HC1_firs_run_lasso_0.01/final_model.csv\"\n",
    "# help_model_file_second_run = f\"{working_dir}/imputed_median_z_global.2_HSCD_HC1_firs_run_lasso_0.01/final_model.csv\"\n",
    "# help_model_file_second_run = f\"{working_dir}/imputed_z_global.2_HSCD_HC1_firs_run_lasso_0.01/final_model.csv\"\n",
    "# help_model_file_second_run = f\"{working_dir}/imputed_z_change_snap_game_true_pos_rt_avrg.0.2_HSCD_HC1_firs_run_lasso_0.01/final_model.csv\"\n",
    "\n",
    "input_file = f\"{working_dir}/data_with_na.csv\"\n",
    "outcome = f\"z_global_change.0.{second_visit}\" # z_global_change.0.2\n",
    "# outcome = f\"z_change_snap_game_true_pos_rt_avrg.0.{second_visit}\"\n",
    "# outcome = f\"z_change_log_pairs_matching_sum_incorrect.0.{second_visit}\"\n",
    "\n",
    "medications =[\"aspirin.0\",  \"anxiety_tr.0\", \"pain_tr.0\", \"TAZD_Thiazide.0\", \"loop_diuretics.0\",  \"potassium_diuretics.0\", \"beta_blockers.0\", \"calcium_antagonists.0\", \n",
    "   \"ARA_II_Antagonists_of_angiotensin_II_receptors.0\", \"IECA_Angiotensin_converting_enzyme_inhibitors.0\",\n",
    "   \"Other_Hypotensive.0\",\"hypochol_statins.0\", \"hypochol_others.0\", \"insulin.0\", \"sulfonylurea.0\", \"thiazolidinediones.0\", \"non_sulfonylurea_insulin_secretagogues.0\", \"metformin_category.0\", \"vitamins_minerals.0\"]\n",
    "\n",
    "df = pd.read_table(input_file, nrows=n_samples, sep=',')\n",
    "\n",
    "if \"imputed\" in input_file:\n",
    "  output_dir = f\"{working_dir}/imputed_{outcome}\"\n",
    "else:\n",
    "  df = df.dropna(axis=\"rows\")\n",
    "  print(f\"cleaned check up db has {len(df)} rows\")\n",
    "  output_dir = f\"{working_dir}/non_na_{outcome}\"\n",
    "  \n",
    "\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "if HSCD is not None:\n",
    "  output_dir = f\"{output_dir}_HSCD_{HSCD}\"\n",
    "\n",
    "'''if help_model_file_second_run is None:\n",
    "  output_dir = f\"{output_dir}_firs_run_lasso_{lasso_epsilon}\"\n",
    "else:\n",
    "  output_dir = f\"{output_dir}_second_run\"'''\n",
    "  \n",
    "if gender is None:\n",
    "  output_dir = f\"{output_dir}/\"\n",
    "else:\n",
    "  output_dir = f\"{output_dir}_gender_{gender}\"\n",
    "  df = df[df['gender.0'].eq(gender)]\n",
    "  df = df.drop(columns=['gender.0'])\n",
    "  predictors.remove(\"gender.0\")\n",
    "  \n",
    "# we need to check for each medications variables if  not all are zeros\n",
    "for medic in medications:\n",
    "  if medic in predictors:\n",
    "    if df[medic].eq(1).sum() == 0:\n",
    "      df = df.drop(columns=[medic])\n",
    "      predictors.remove(medic)\n",
    "      print(f\"dropped column because all zeroes: {medic}\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "  os.makedirs(output_dir)\n",
    "\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if memory_experiment == \"nm\":\n",
    "    memory_experiment_full_name = \"num_memory_max_digits_remembered_correctly\"\n",
    "else:\n",
    "    memory_experiment_full_name = \"log_pairs_matching_sum_incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''predictors = [\"gender.0\", f\"log_time_difference.0.{second_visit}\",\"age_years.0\", \"education_level.0\", \n",
    "              \"learning_effect_prior.2\",  \"Diabetes_2.0\",  \"Dyslipidemia.0\", \"Depression.0\", \"Hypertension.0\",\n",
    "              \"Syst_bp.0\", \"log_hdl_conv.0\", \"log_Triglycerides_conv.0\", \"log_HbA1c_conv.0\",  \n",
    "              \"log_C_reactive_protein.0\"]'''\n",
    "'''\"log_oily_fish_gpday_bradbury.0\", \"log_white_fish_gpday_bradbury.0\", \n",
    "   \"log_red_meat_bradbury_gpd.0\", \"log_poultry_gpday_bradbury.0\",\n",
    "   \"log_processed_meat_gpday_bradbury.0\", \"log_veg_gpday_bradbury.0\",\n",
    "   \"log_fruit_gpday_bradbury.0\",\n",
    "    \"log_cereals_gpday_bradbury.0\", \"log_bread_gpday_bradbury.0\", \"log_cheese_gpday_bradbury.0\",\n",
    "    \"log_milk_gpday_bradbury.0\", \"log_tea_gpday_bradbury.0\",\n",
    "    \"quadr_red_wine_gpd.0\", \"quadr_white_wine_gpd.0\", \"quadr_fortified_gpd.0\", \"quadr_beer_cider_gpd.0\", \"quadr_spirits_gpd.0\"]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[predictors+[outcome]]\n",
    "if remove_na:\n",
    "    df = df.dropna(axis=\"rows\")\n",
    "    print(f\"cleaned check up db has {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_predictors =[\"age_years.0\", \"gender.0\", \"education_level.0\", f\"log_time_difference.0.{second_visit}\"]\n",
    "if (gender is not None) and (\"gender.0\") in simple_predictors:\n",
    "    simple_predictors.remove(\"gender.0\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_predictors = predictors\n",
    "for predictor in global_predictors:\n",
    "    if predictor not in df.columns:\n",
    "        print(f\"predictor column  is missing {predictor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(df_source, predictors, outcome_var, result_dir, hcd):\n",
    "    \n",
    "    if not os.path.exists(result_dir):\n",
    "    # The directory does not exist, create it\n",
    "        os.makedirs(result_dir)\n",
    "        \n",
    "    y = df_source[outcome_var]\n",
    "    print(f\"outcome: {y}\")\n",
    "    \n",
    "    X = df_source[predictors]\n",
    "        \n",
    "  \n",
    "    # getting basic statistics for all predictors and standartising on the flow\n",
    "   \n",
    "    df_mean_std = pd.DataFrame(columns = [\"predictor\", \"mean\", \"standard_deviation\", \"min\", \"max\"] )\n",
    "    \n",
    "    predictors = X.columns \n",
    "    for i in  range(len(predictors)):  \n",
    "        clmn = predictors[i]\n",
    "        mean_ = X[clmn].mean()\n",
    "        std_ = X[clmn].std()\n",
    "        df_mean_std.loc[i] = [clmn, mean_, std_, X[clmn].min(), X[clmn].max()]\n",
    "        X[clmn] = (X[clmn] - mean_)/std_\n",
    "        \n",
    "   \n",
    "    X_with_intercept = sm.add_constant(X) \n",
    "    if hcd is not None:\n",
    "        model_1= sm.OLS(y, X_with_intercept).fit(cov_type=hcd) \n",
    "    else:\n",
    "        model_1= sm.OLS(y, X_with_intercept).fit() \n",
    "    \n",
    "    \n",
    "    with open(f\"{result_dir}/ols_model_summary.txt\", 'w') as model_1_file:\n",
    "    # Convert the summary to a string and write it to the file\n",
    "        model_1_file.write(model_1.summary().as_text())\n",
    "    \n",
    "    coefficients = model_1.params\n",
    "    p_values = model_1.pvalues\n",
    "    df_results = pd.DataFrame({'Coefficient': coefficients, 'P_value': p_values})\n",
    "    \n",
    "    df_results = df_results.reset_index()\n",
    "    df_results = df_results.rename(columns={df_results.columns[0]: 'predictor'})\n",
    "    df_results = pd.merge(df_results, df_mean_std, on = \"predictor\", how = 'outer')\n",
    "    \n",
    "    df_results.to_csv(f\"{result_dir}/final_ols_model.csv\", sep=',')\n",
    "  \n",
    "    return df_results, model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_model_builder(df_source, predictors, outcome_var, result_dir, hcd):\n",
    "    \n",
    "    if not os.path.exists(result_dir):\n",
    "    # The directory does not exist, create it\n",
    "        os.makedirs(result_dir)\n",
    "        \n",
    "    y = df_source[outcome_var]\n",
    "    print(f\"outcome: {y}\")\n",
    "    \n",
    "    X = df_source[predictors]\n",
    "   \n",
    "    \n",
    "    ############ LASSO ###### \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    # Initialize LassoCV with 100-fold cross-validation\n",
    "    lasso_cv = LassoCV(cv=100, random_state=42, max_iter=10000)\n",
    "    pipeline = make_pipeline(scaler, lasso_cv)\n",
    "    pipeline.fit(X, y)\n",
    "        \n",
    "    lasso_cv_fitted = pipeline.named_steps['lassocv']\n",
    "    \n",
    "    \n",
    "\n",
    "    summary_text = f\"Best alpha (regularization strength): {lasso_cv_fitted.alpha_}\\n\"\n",
    "    summary_text += f\"Intercept {lasso_cv_fitted.intercept_}\\nCoefficients:\\n\"\n",
    "    summary_text += f\"{lasso_cv_fitted.coef_}\\n\"\n",
    "    summary_text += f\"MSE path {lasso_cv_fitted.mse_path_}\\n\"\n",
    "        \n",
    "    r2_score = pipeline.score(X, y)\n",
    "    summary_text += f\"R^2 score: {r2_score}\"\n",
    "         \n",
    "    print(summary_text)\n",
    "        # Saving to a file\n",
    "    with open(f\"{result_dir}/lasso_summary.txt\", 'w') as lasso_file:\n",
    "        lasso_file.write(summary_text)\n",
    "    \n",
    "    # preparing the list of predictors with non zero coefficients (large enough)\n",
    "    if abs(lasso_cv_fitted.intercept_) >=  lasso_epsilon:\n",
    "        selected_features = [\"const\"]  \n",
    "    else:  \n",
    "        selected_features = []\n",
    "        \n",
    "    for i in range(len(predictors)):\n",
    "        if abs(lasso_cv_fitted.coef_[i])>= lasso_epsilon:\n",
    "            selected_features = selected_features + [predictors[i]]\n",
    "   \n",
    "    \n",
    "    # getting coefficients and basic statistics for all predictors, incl const\n",
    "   \n",
    "    df_mean_std = pd.DataFrame(columns = [\"predictor\", \"Coefficient\", \"mean\", \"standard_deviation\", \"min\", \"max\"] )\n",
    "    df_mean_std.loc[0] = [\"const\", lasso_cv_fitted.intercept_, lasso_cv_fitted.intercept_, 0, lasso_cv_fitted.intercept_, lasso_cv_fitted.intercept_]\n",
    "    X_scaled = df_source[predictors]\n",
    "    for i in  range(len(predictors)):  \n",
    "        clmn = predictors[i]\n",
    "        print(clmn)\n",
    "        print(f\"how many na we have prior standartisation: {X_scaled[clmn].isna().sum()}\")\n",
    "        mean_ = X_scaled[clmn].mean()\n",
    "        std_ = X_scaled[clmn].std()\n",
    "        df_mean_std.loc[i+1] = [clmn, lasso_cv_fitted.coef_[i], mean_, std_, X_scaled[clmn].min(), X_scaled[clmn].max()]\n",
    "        X_scaled[clmn] =  (X_scaled[clmn] - mean_)/std_\n",
    "        print(f\"how many na we have after standartisation: {X_scaled[clmn].isna().sum()}\")\n",
    "        \n",
    "    ## getting p-values for all predictors via ols model\n",
    "   \n",
    "    X_scaled_with_intercept = sm.add_constant(X_scaled) \n",
    "    if hcd is not None:\n",
    "        model_1 = sm.OLS(y, X_scaled_with_intercept).fit(cov_type=hcd) \n",
    "    else:\n",
    "        model_1 = sm.OLS(y, X_scaled_with_intercept).fit() \n",
    "    \n",
    "    \n",
    "    with open(f\"{result_dir}/ols_model_summary.txt\", 'w') as model_1_file:\n",
    "    # Convert the summary to a string and write it to the file\n",
    "        model_1_file.write(model_1.summary().as_text())\n",
    "    \n",
    "    coefficients = model_1.params\n",
    "    p_values = model_1.pvalues\n",
    "    df_results = pd.DataFrame({'OLS_Coefficient': coefficients, 'P_value': p_values})\n",
    "    \n",
    "    df_results = df_results.reset_index()\n",
    "    df_results = df_results.rename(columns={df_results.columns[0]: 'predictor'})\n",
    "    df_results = pd.merge(df_results, df_mean_std, on = \"predictor\", how = 'outer')\n",
    "    \n",
    "    df_results.to_csv(f\"{result_dir}/lasso_eval_model.csv\", sep=',')\n",
    "    \n",
    "    if 'const' in selected_features:\n",
    "        selected_features.remove('const')\n",
    "        \n",
    "    X_selected = X_scaled[selected_features]\n",
    "    X_selected_with_intercept = sm.add_constant(X_selected) \n",
    "    if hcd is not None:\n",
    "        model_2= sm.OLS(y, X_selected_with_intercept).fit(cov_type=hcd) \n",
    "    else:\n",
    "        model_2= sm.OLS(y, X_selected_with_intercept).fit() \n",
    "        \n",
    "    with open(f\"{result_dir}/final_model_summary.txt\", 'w') as model_2_file:\n",
    "    # Convert the summary to a string and write it to the file\n",
    "        model_2_file.write(model_2.summary().as_text())\n",
    "        \n",
    "    print(model_2.summary())\n",
    "    coefficients_2 = model_2.params\n",
    "    p_values_2 = model_2.pvalues\n",
    "    df_selected_results = pd.DataFrame({'OLS_Coefficient': coefficients_2, 'P_value': p_values_2})\n",
    "    \n",
    "    df_selected_results = df_selected_results.reset_index()\n",
    "    df_selected_results = df_selected_results.rename(columns={df_selected_results.columns[0]: 'predictor'})\n",
    "    df_selected_results = pd.merge(df_selected_results, df_mean_std, on = \"predictor\", how = 'outer')\n",
    "    \n",
    "    df_selected_results = df_selected_results[df_selected_results['predictor'].isin(selected_features + ['const'])]\n",
    "    df_selected_results.to_csv(f\"{result_dir}/final_model_with_lasso.csv\", sep=',')\n",
    "    \n",
    "    return df_selected_results, model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dependencies(df_, model_df, predictors_to_show, dir):\n",
    "    \n",
    "    if model_df is None:\n",
    "        return\n",
    "    \n",
    "    if os.path.exists(dir):\n",
    "        shutil.rmtree(dir)\n",
    "    os.makedirs(dir)\n",
    "    \n",
    "    print(\"we are building graphs\")\n",
    "    \n",
    "    for predictor in predictors_to_show:\n",
    "        \n",
    "        \n",
    "        print(predictor)\n",
    "       \n",
    "        y = np.zeros(df_.shape[0])\n",
    "        # \n",
    "        # aggregate all the terms containing pedictor \n",
    "        for feat in model_df['predictor']:\n",
    "            if not predictor in feat:\n",
    "                continue\n",
    "            if not feat in df_.columns:\n",
    "                continue\n",
    "            print(feat)\n",
    "            \n",
    "            # prepare the feature\n",
    "            match feat:\n",
    "                case _ if feat == predictor:\n",
    "                    fun_pred = df_[predictor]\n",
    "                case _ if feat == f\"log_{predictor}\":\n",
    "                    fun_pred = np.log1p(df_[predictor])\n",
    "                case _ if feat == f\"quadr_{predictor}\":\n",
    "                    fun_pred = np.square(df_[predictor])\n",
    "                case _ if feat == f\"quadr_log_{predictor}\":\n",
    "                    fun_pred = np.square(np.log1p(df_[predictor]))\n",
    "                case _:\n",
    "                    fun_pred = None\n",
    "                    print(f\"error, non existing predictor {feat}\")\n",
    "            # standartise the feature\n",
    "            fun_pred = (fun_pred - fun_pred.mean())/fun_pred.std()\n",
    "            \n",
    "            \n",
    "            \n",
    "            # find the coefficient\n",
    "            row_feat = model_df[model_df['predictor'] == feat].iloc[0]\n",
    "            coeff =  row_feat[\"Coefficient\"]\n",
    "            \n",
    "            y = y + coeff *fun_pred\n",
    "        \n",
    "        # x_pred = (df_[predictor]    - df_[predictor].mean()) /  df_[predictor].std()\n",
    "        # x = np.linspace(min(x_pred), max(x_pred), 100)  \n",
    "          \n",
    "        are_all_zeros = np.all(y == 0)        \n",
    "        if not are_all_zeros:\n",
    "            x = df_[predictor]\n",
    "        \n",
    "            plt.scatter(x, y)\n",
    "            plt.xlabel(predictor)\n",
    "            plt.ylabel('outcome')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "            # Save the plot as a PNG file\n",
    "            plt.savefig(f\"{dir}/{predictor}.png\", format='png', dpi=300)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lasso_epsilon is not None :\n",
    "        \n",
    "    model_table, model = lasso_model_builder(df, global_predictors, outcome, f\"{output_dir}\",  HSCD)\n",
    "    model_table_simple, model_simple = model_builder(df, simple_predictors, outcome, f\"{output_dir}\",  HSCD)\n",
    "            \n",
    "else:\n",
    "    print(simple_predictors)\n",
    "    X = df[global_predictors]\n",
    "    y = df[outcome]\n",
    "    assert(len(X) == len(y))\n",
    "    \n",
    "    duplicate_indices_X = X.index.duplicated().any()\n",
    "    duplicate_indices_y = y.index.duplicated().any()\n",
    "\n",
    "    print(f\"Does X have duplicate indices? {duplicate_indices_X}\")\n",
    "    print(f\"Does y have duplicate indices? {duplicate_indices_y}\")\n",
    "    \n",
    "    indices_aligned = X.index.equals(y.index)\n",
    "\n",
    "    print(f\"Are the indices of X and y aligned? {indices_aligned}\") \n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "    \n",
    "    assert(len(X_train) == len(y_train))\n",
    "    assert(len(X_val) == len(y_val))\n",
    "    \n",
    "    '''\n",
    "    X_train, X_v0, y_train, y_v0 = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "    \n",
    "    assert(len(X_train) == len(y_train))\n",
    "    assert(len(X_v0) == len(y_v0))\n",
    "    \n",
    "    \n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_v0, y_v0, test_size=0.5, random_state=1)\n",
    "    assert(len(X_val) == len(y_val))\n",
    "    assert(len(X_test) == len(y_test))\n",
    "    '''\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # main model    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    # show_dependencies(df, model_table, predictors_to_plot, f\"{output_dir}/figures\")\n",
    "  \n",
    "    \n",
    "    y_pred_val = model.predict(X_val_scaled)\n",
    "   \n",
    "    mae = mean_absolute_error(y_val, y_pred_val)\n",
    "    mse = mean_squared_error(y_val, y_pred_val)\n",
    "    rmse = mean_squared_error(y_val, y_pred_val, squared=False)  # RMSE is the square root of MSE\n",
    "    r2 = r2_score(y_val, y_pred_val)\n",
    "    \n",
    "    validation_summary = f\"vaidation mean_absolute_error  { mae}\\n\"\n",
    "    validation_summary += f\"vaidationroot mean_squared_error  {rmse}\\n\"\n",
    "    validation_summary += f\"vaidation R-squared (the coefficient of determination) {r2}\\n\"\n",
    "    \n",
    "    \n",
    "    # simple model      \n",
    "    X_train_simple = X_train[simple_predictors]\n",
    "    X_val_simple = X_val[simple_predictors]\n",
    "    scaler_simple = StandardScaler()\n",
    "    X_train_simple_scaled = scaler_simple.fit_transform(X_train_simple)\n",
    "    X_val_simple_scaled = scaler_simple.transform(X_val_simple)\n",
    "    \n",
    "    model_simple = LinearRegression()\n",
    "    model_simple.fit(X_train_simple_scaled, y_train)\n",
    "       \n",
    "    \n",
    "    y_simple_pred_val = model_simple.predict(X_val_simple_scaled)\n",
    "    mae_simple = mean_absolute_error(y_val, y_simple_pred_val)\n",
    "    mse_simple = mean_squared_error(y_val, y_simple_pred_val)\n",
    "    rmse_simple = mean_squared_error(y_val, y_simple_pred_val, squared=False)  # RMSE is the square root of MSE\n",
    "    r2_simple = r2_score(y_val, y_simple_pred_val)\n",
    "    \n",
    "    validation_summary += f\"vaidation simple model mean_absolute_error  { mae_simple}\\n\"\n",
    "    validation_summary += f\"vaidation simple model root mean_squared_error  {rmse_simple}\\n\"\n",
    "    validation_summary += f\"vaidation simple model R-squared (the coefficient of determination) {r2_simple}\\n\"\n",
    "    \n",
    "    print(validation_summary)\n",
    "    with open(f\"{output_dir}/validation_summary.txt\", 'w') as lasso_file:\n",
    "        lasso_file.write(validation_summary)\n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copula",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
